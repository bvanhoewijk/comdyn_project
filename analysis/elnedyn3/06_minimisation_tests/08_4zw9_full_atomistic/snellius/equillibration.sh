#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=48
#SBATCH --partition=genoa
#SBATCH --time=8:00:00
module load 2023
module load GROMACS/2023.3-foss-2023a

# Generated by CHARMM-GUI (http://www.charmm-gui.org) v3.7

# This folder contains GROMACS formatted force fields, a pre-optimized PDB structure, and GROMACS inputs.
# All input files were optimized for GROMACS 2019.2 or above, so lower version of GROMACS can cause some errors.
# We adopted the Verlet cut-off scheme for all minimization, equilibration, and production steps because it is 
# faster and more accurate than the group scheme. If you have a trouble with a performance of Verlet scheme while 
# running parallelized simulation, you should check if you are using appropriate command line.
# For MPI parallelizing, we recommand following command:
# mpirun -np $NUM_CPU gmx mdrun -ntomp 1

init="step5_input"
rest_prefix="step5_input"
mini_prefix="step6.0_minimization"
equi_prefix="step6.%d_equilibration"
prod_prefix="step7_production"
prod_step="step7"

# Minimization
# In the case that there is a problem during minimization using a single precision of GROMACS, please try to use 
# a double precision of GROMACS only for the minimization step.
srun gmx_d grompp -f "${mini_prefix}.mdp" -o "${mini_prefix}.tpr" -c "${init}.gro" -r "${rest_prefix}.gro" -p topol.top -n index.ndx
srun gmx_d mdrun -v -deffnm "${mini_prefix}" -nt 48 -pin on

# Equilibration
cnt=1
cntmax=6

while [ $cnt -le $cntmax ]
do
    pcnt=$((cnt - 1))
    istep=$(printf "$equi_prefix" $cnt)
    pstep=$(printf "$equi_prefix" $pcnt)
    if [ $cnt -eq 1 ]; then
        pstep="$mini_prefix"
    fi

    srun gmx grompp -f "${istep}.mdp" -o "${istep}.tpr" -c "${pstep}.gro" -r "${rest_prefix}.gro" -p topol.top -n index.ndx
    srun gmx mdrun -v -deffnm "$istep" -nt 48 -pin on
    cnt=$((cnt + 1))
done
